{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Katcher's dataset for RLMREC\n",
    "\n",
    "Note: This notebook is ad-hoc based. The paths of the necessary files are hardcoded. Sorry for the inconvenience.\n",
    "\n",
    "Input\n",
    "- train_LLM.csv, val_LLM.csv, test_LLM.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from scipy.sparse import coo_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data = pd.read_csv('/Jupyter/dev_src/RLMRec/data/katchers/raw/train_LLM.csv')\n",
    "raw_val_data = pd.read_csv('/Jupyter/dev_src/RLMRec/data/katchers/raw/val_LLM.csv')\n",
    "raw_test_data = pd.read_csv('/Jupyter/dev_src/RLMRec/data/katchers/raw/test_LLM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make item dictionary \n",
    "\n",
    "iid, prod_id, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_each_row(row):\n",
    "    pattern = r'주문 내역: (.+?)(?=\\d{4}-\\d{2}-\\d{2}|\\Z)'\n",
    "    prev = row.prev\n",
    "    matches = re.findall(pattern, prev, re.DOTALL)\n",
    "    x = [match.strip() for match in matches]\n",
    "    y = [int(y_) for y_ in row.prev_id.split(',')]\n",
    "    x_next = row.next.split('\\n')[1]\n",
    "    y_next = row.next_id\n",
    "    x.append(x_next)\n",
    "    y.append(y_next)\n",
    "    return {id:ss for id, ss in zip(y, x)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dic = {}\n",
    "for d in raw_train_data.apply(convert_each_row, axis=1):\n",
    "    item_dic.update(d)\n",
    "for d in raw_val_data.apply(convert_each_row, axis=1):\n",
    "    item_dic.update(d)\n",
    "for d in raw_test_data.apply(convert_each_row, axis=1):\n",
    "    item_dic.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = pd.DataFrame(list(item_dic.items()), columns=['prod_id', 'title']).sort_values(by='prod_id').reset_index(drop=True)\n",
    "item_df.index.name = 'iid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output: katchers_item.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uniq items: 1,669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.reset_index()[['iid','prod_id']].to_json('/Jupyter/dev_src/RLMRec/data/mapper/katchers_item.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.to_pickle('/Jupyter/dev_src/RLMRec/data/katchers/intermediate/item_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make user dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat user columns\n",
    "all_users = pd.concat([raw_train_data['user'], raw_val_data['user'], raw_test_data['user']])\n",
    "\n",
    "# get unique users\n",
    "unique_users = sorted(all_users.unique())\n",
    "\n",
    "# DataFrame \n",
    "user_df = pd.DataFrame({\n",
    "    'uid': range(len(unique_users)),\n",
    "    'user_id': unique_users,\n",
    "    \n",
    "})\n",
    "\n",
    "user_df.set_index('uid', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output: katchers_user.json\n",
    "\n",
    "uniq users: 16,813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.reset_index()[['uid','user_id']].to_json('/Jupyter/dev_src/RLMRec/data/mapper/katchers_user.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.to_pickle('/Jupyter/dev_src/RLMRec/data/katchers/intermediate/user_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16813, 1669)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_df), len(item_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathers purchase history in order to build user profile\n",
    "\n",
    "\n",
    "Note: The resulting history file (compact_history_df.pkl) is used as a source of GPT input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.concat((raw_train_data.query(\"label==1\"),\n",
    "raw_val_data.query(\"label==1\"),\n",
    "raw_test_data.query(\"label==1\")), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_history_df = pd.merge(history_df, user_df.reset_index(), left_on='user', right_on='user_id').set_index('uid')[['user', 'prev_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_title_dic = item_df.set_index('prod_id')['title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_history_mapper(row):\n",
    "    user = row.user\n",
    "    purchased_prods = []\n",
    "    for prod in set(row.prev_id.split(',')):\n",
    "        purchased_prods.append(\n",
    "            {'title':prod_title_dic[int(prod)],\n",
    "         'description':'None',\n",
    "         'review':'None'\n",
    "        }\n",
    "        )\n",
    "    json_str = json.dumps(purchased_prods, ensure_ascii=False)\n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_history_df['user_profile_input'] = compact_history_df.apply(item_history_mapper, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output: intermediate/compact_history_df.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_history_df.to_pickle('/Jupyter/dev_src/RLMRec/data/katchers/intermediate/compact_history_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make coo_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prev_id in the purchase history is used to build the coo_matrix.\n",
    "\n",
    "The prev_is of users in the validation and test are integrated into the training matrix.\n",
    "\n",
    "However the next_id of users in the validation and test are kept for evaluation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16813 1669\n"
     ]
    }
   ],
   "source": [
    "n_user, n_item = len(user_df), len(item_df)\n",
    "print(n_user, n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_history_df = pd.concat((raw_train_data.query(\"label==1\"),\n",
    "raw_val_data.query(\"label==1\"),\n",
    "raw_test_data.query(\"label==1\")), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_to_iid_dic = item_df.reset_index().set_index('prod_id')['iid'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purchased_prev_iids_wrapper(row):\n",
    "    user = row.user\n",
    "    \n",
    "    purchased_prods = set([int(x) for x in row.prev_id.split(',')])\n",
    "\n",
    "    purchased_iids = []\n",
    "    for prod in purchased_prods:\n",
    "        if prod in prod_to_iid_dic:\n",
    "            purchased_iids.append(prod_to_iid_dic[prod])\n",
    "\n",
    "    return purchased_iids\n",
    "\n",
    "def purchased_next_id_wrapper(row):\n",
    "    user = row.user\n",
    "    \n",
    "    purchased_prods = set([int(row.next_id)])\n",
    "\n",
    "    purchased_iids = []\n",
    "    for prod in purchased_prods:\n",
    "        if prod in prod_to_iid_dic:\n",
    "            purchased_iids.append(prod_to_iid_dic[prod])\n",
    "\n",
    "    return purchased_iids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_train_coo_matrix(in_df, user_df, n_user, n_items):\n",
    "    tmp_df = pd.merge(in_df.query(\"label==1\")[['user','prev_id']], user_df.reset_index(), left_on='user', right_on='user_id').set_index('uid')\n",
    "    tmp_df['iids'] = tmp_df.apply(purchased_prev_iids_wrapper, axis=1)\n",
    "    v = tmp_df.explode('iids')['iids']\n",
    "    return coo_matrix(([1.0] * len(v), (v.index.values, v.values)), shape=(n_user, n_item), dtype=np.float32)\n",
    "\n",
    "def build_other_coo_matrix(in_df, user_df, n_user, n_items):\n",
    "    tmp_df = pd.merge(in_df.query(\"label==1\")[['user','next_id']], user_df.reset_index(), left_on='user', right_on='user_id').set_index('uid')\n",
    "    tmp_df['iids'] = tmp_df.apply(purchased_next_id_wrapper, axis=1)\n",
    "    v = tmp_df.explode('iids')['iids']\n",
    "    return coo_matrix(([1.0] * len(v), (v.index.values, v.values)), shape=(n_user, n_item), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_mat = build_train_coo_matrix(in_df=all_history_df, user_df=user_df, n_user=n_user, n_items=n_item)\n",
    "with open('/Jupyter/dev_src/RLMRec/data/katchers/trn_mat.pkl', 'wb') as f:\n",
    "    pickle.dump(trn_mat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mat = build_other_coo_matrix(in_df=raw_val_data, user_df=user_df, n_user=n_user, n_items=n_item)\n",
    "with open('/Jupyter/dev_src/RLMRec/data/katchers/val_mat.pkl', 'wb') as f:\n",
    "    pickle.dump(val_mat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_mat = build_other_coo_matrix(in_df=raw_test_data, user_df=user_df, n_user=n_user, n_items=n_item)\n",
    "with open('/Jupyter/dev_src/RLMRec/data/katchers/tst_mat.pkl', 'wb') as f:\n",
    "    pickle.dump(tst_mat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = list(zip(trn_mat.row, trn_mat.col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make extra information for test data\n",
    "\n",
    "In our paper, during testing, each user is assigned one positive example and 49 negative examples. We measured NDCG@K based on their predicted values. Similarly, for RMLEC, we generated tst_masked_unquery_iids information to evaluate 50 examples per test user. Since RMLEC performs predictions on all items, we added this masking information to ensure that only the predicted values for the 50 test examples are used, and the others are disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.merge(raw_test_data[['user','next_id']], user_df.reset_index(), left_on='user', right_on='user_id').set_index('uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df['test_iid'] = tmp_df.apply(lambda row: prod_to_iid_dic[int(row.next_id)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iids = set(item_df.index.values)\n",
    "test_query_iids_dic = tmp_df.reset_index().groupby('uid')['test_iid'].apply(set)\n",
    "masked_unquery_iids = {uid:list(all_iids - query_iids) for uid, query_iids in test_query_iids_dic.items()}\n",
    "with open('/Jupyter/dev_src/RLMRec/data/katchers/tst_maksed_unquery_iids.pkl', 'wb') as f:\n",
    "    pickle.dump(masked_unquery_iids, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
